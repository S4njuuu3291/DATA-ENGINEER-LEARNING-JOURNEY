# Data Serialization Contracts

## ğŸ“‹ Overview

Repository ini berisi teori dan praktik tentang **Avro serialization** untuk Data Engineering, dengan fokus pada:
1. **Batch Processing** (Airflow)
2. **In-Memory Serialization** (JSON â†’ Avro)
3. **Multi-Destination Distribution** (S3, BigQuery, PostgreSQL, dll)

---

## ğŸ“ Struktur Folder

```
Data_Serialization_Contracts/
â”œâ”€â”€ 1-teori-dasar/
â”‚   â”œâ”€â”€ 1-teori-dasar.md                    # Filosofi Avro (basic)
â”‚   â”œâ”€â”€ 2-avro-batch-airflow.md             # Teori Avro untuk batch processing
â”‚   â”œâ”€â”€ 2-avro-inmemory-serialize.py        # Praktik in-memory serialization
â”‚   â”œâ”€â”€ 3-airflow-batch-etl-example.py      # Contoh lengkap Airflow DAG
â”‚   â”œâ”€â”€ 4-extensible-schema-registry.py     # Scalable schema management
â”‚   â”œâ”€â”€ trade_schema.avsc                   # Schema file (Legacy)
â”‚   â”œâ”€â”€ trade_schema_v2.avsc                # Schema file v2 (Legacy)
â”‚   â”œâ”€â”€ trades.avro                         # Sample Avro file (Legacy)
â”‚   â”œâ”€â”€ 1-test_avro.py                      # Test basic (Legacy)
â”‚   â””â”€â”€ schemas/                            # New: Extensible schema storage
â”‚       â”œâ”€â”€ crypto_trade.json
â”‚       â”œâ”€â”€ price_aggregate.json
â”‚       â”œâ”€â”€ batch_metadata.json
â”‚       â”œâ”€â”€ data_quality.json
â”‚       â””â”€â”€ user_activity.json
â”‚
â””â”€â”€ README.md (this file)

```

---

## ğŸ¯ Learning Path

### Tahap 1: Pemahaman Fundamental
- **File:** `1-teori-dasar.md`
- **Topik:** Mengapa Avro? Masalah JSON, perbandingan binary vs textual
- **Durasi:** 30 menit

### Tahap 2: Avro untuk Batch Processing
- **File:** `2-avro-batch-airflow.md`
- **Topik:** Konteks Airflow, in-memory serialization, multi-destination pattern
- **Durasi:** 45 menit

### Tahap 3: Praktik In-Memory Serialization
- **File:** `2-avro-inmemory-serialize.py`
- **Topik:** Code examples untuk serialize JSON â†’ Avro bytes
- **Jalankan:** `python 2-avro-inmemory-serialize.py`

### Tahap 4: Airflow ETL Pipeline
- **File:** `3-airflow-batch-etl-example.py`
- **Topik:** Contoh lengkap DAG simulation dengan multiple destinations
- **Jalankan:** `python 3-airflow-batch-etl-example.py`

### Tahap 5: Schema Management (Production-Ready)
- **File:** `4-extensible-schema-registry.py`
- **Topik:** Scalable schema registry, mudah di-extend
- **Jalankan:** `python 4-extensible-schema-registry.py`

---

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install fastavro
```

### 2. Run Examples
```bash
# In-memory serialization
python 2-avro-inmemory-serialize.py

# Airflow DAG simulation
python 3-airflow-batch-etl-example.py

# Schema registry
python 4-extensible-schema-registry.py
```

### 3. Output
Setiap script akan menampilkan:
- Data transformation steps
- Performance metrics (compression ratio, speed)
- Sample outputs

---

## ğŸ—ï¸ Patterns & Best Practices

### Pattern 1: Serialize Once, Use Many
```python
# Extract & Serialize (once)
avro_bytes = AvroSerializer.serialize(json_data, schema)

# Use for multiple destinations
save_to_s3(avro_bytes)
save_to_bigquery(avro_bytes)
save_to_postgres(avro_bytes)
```

**Keuntungan:**
- 1 serialization, 3 destinations
- Konsisten across all targets
- CPU-efficient

### Pattern 2: Airflow XCom Communication
```python
# Task 1: Extract & Serialize
avro_bytes = serialize(data)
ti.xcom_push(key='avro_data', value=avro_bytes)

# Task 2: Validate & Process
avro_bytes = ti.xcom_pull(task_ids='task_1')
data = deserialize(avro_bytes)
validate(data)

# Task 3: Distribute
for destination in [s3, bq, pg]:
    destination.write(avro_bytes)
```

### Pattern 3: Extensible Schema Registry
```python
# Setup (once)
registry = ExtensibleSchemaRegistry("./schemas")

# Use throughout pipeline
schema = registry.get_schema("crypto_trade")
avro_bytes = serialize(data, schema)

# Easy to extend: just add new .json file to schemas/
```

---

## ğŸ“Š Performance Comparison

| Metrik | JSON | Avro | Improvement |
|--------|------|------|------------|
| **Ukuran (1M records)** | ~200 MB | ~50 MB | 75% smaller |
| **Serialize** | 2s | 0.5s | 4x faster |
| **Deserialize** | 3s | 0.3s | 10x faster |
| **Schema Validation** | Manual | Built-in | Automatic |
| **Type Safety** | Weak | Strong | Type-safe |

---

## ğŸ”„ Avro vs JSON: Kapan Gunakan Apa?

### Gunakan Avro âœ…
1. **Multi-destination pipelines** â†’ Schema adalah kontrak
2. **Batch processing besar** â†’ Performance & storage
3. **Data governance** â†’ Enforce struktur sejak awal
4. **Compression needed** â†’ Binary 75% lebih kecil
5. **Cloud native** â†’ BigQuery, S3 native support

### Gunakan JSON âœ…
1. **Prototype/debug** â†’ Human-readable
2. **Ad-hoc queries** â†’ Fleksibilitas
3. **Simple CSV exports** â†’ Overhead tidak worth it
4. **API responses** â†’ Standard for web

---

## ğŸ› ï¸ Integration dengan Tools

### Airflow
```python
from airflow import DAG
from airflow.operators.python import PythonOperator

def extract_task(**context):
    data = api.get_data()
    avro_bytes = AvroSerializer.serialize(data, schema)
    context['task_instance'].xcom_push(key='avro_data', value=avro_bytes)

def transform_task(**context):
    avro_bytes = context['task_instance'].xcom_pull(task_ids='extract')
    data = AvroSerializer.deserialize(avro_bytes, schema)
    # Process...
```

### Spark
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("AvroExample").getOrCreate()

# Read Avro
df = spark.read.format("avro").load("s3://bucket/data.avro")

# Write Avro
df.write.format("avro").save("s3://bucket/output.avro")
```

### BigQuery
```python
from google.cloud import bigquery

bq_client = bigquery.Client()

# Avro data bisa langsung di-load
job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.AVRO
)
```

---

## ğŸ“š Schema Versioning (untuk production)

Avro mendukung schema evolution:

```python
# Schema v1
v1_schema = {
    "type": "record",
    "name": "Trade",
    "fields": [
        {"name": "symbol", "type": "string"},
        {"name": "price", "type": "double"},
    ]
}

# Schema v2 (backward compatible)
v2_schema = {
    "type": "record",
    "name": "Trade",
    "fields": [
        {"name": "symbol", "type": "string"},
        {"name": "price", "type": "double"},
        {"name": "quantity", "type": "double", "default": 0},  # New field dengan default
    ]
}

# v1 data bisa dibaca dengan v2 schema (backward compatibility)
```

---

## ğŸ” Data Governance Checklist

- [x] Schema defined & versioned
- [x] In-memory serialization tested
- [x] Multi-destination working
- [x] Validation before write
- [x] Error handling implemented
- [x] Performance benchmarked
- [ ] Production deployment
- [ ] Monitoring & alerts
- [ ] Disaster recovery plan

---

## ğŸ“ Notes

### Legacy Files
- `1-test_avro.py` - Basic test (dari workshop original)
- `trade_schema.avsc` - Sample schema file
- `trades.avro` - Sample data file

Semua contoh baru focus pada **in-memory serialization** yang lebih cocok untuk Airflow.

### Next Steps
1. Integrate ke Airflow DAG
2. Test dengan real data
3. Setup monitoring
4. Document data contracts

---

## ğŸ“ References

- **Avro Spec:** https://avro.apache.org/docs/current/
- **FastAvro:** https://github.com/fastavro/fastavro
- **Airflow XCom:** https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html

---

**Last Updated:** 2024-01-29  
**Author:** Data Engineering Team
