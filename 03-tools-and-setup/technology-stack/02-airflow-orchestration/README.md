# ğŸš€ Apache Airflow - Workflow Orchestration

Materi lengkap untuk menguasai **Apache Airflow** dalam Data Engineering - dari konsep dasar sampai production-ready practices.

## ğŸ¯ Kenapa Airflow?

Airflow adalah **workflow orchestration tool** yang paling populer di data engineering untuk:
- âœ… Schedule & monitoring ETL/ELT pipelines
- âœ… Manage dependencies antar tasks
- âœ… Retry mechanism otomatis
- âœ… Visualisasi workflow (DAG)
- âœ… Scalable & production-ready

---

## ğŸ“š Learning Path

### ğŸ”´ **CRITICAL: Core Concepts** (Harus dikuasai!)

#### **1. DAG Fundamentals**
Folder: `1-dag-fundamentals/`

**Topics:**
- âœ… DAG (Directed Acyclic Graph) structure
- âœ… Task dependencies (`>>`, `<<`, `set_upstream`, `set_downstream`)
- âœ… Task scheduling & intervals
- âœ… Basic operators (PythonOperator, BashOperator)

**Learn:**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator

with DAG(
    dag_id='my_first_dag',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    task1 = PythonOperator(task_id='extract', python_callable=extract_data)
    task2 = PythonOperator(task_id='transform', python_callable=transform_data)
    task3 = PythonOperator(task_id='load', python_callable=load_data)
    
    task1 >> task2 >> task3  # Dependencies
```

---

#### **2. TaskFlow API** â­ Modern Approach
Folder: `2-taskflow-api/`

**Topics:**
- âœ… `@task` decorator (cleaner syntax)
- âœ… XCom communication (auto return values)
- âœ… Type hints & automatic serialization
- âœ… Error handling dalam tasks

**Learn:**
```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='taskflow_example',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
)
def my_etl_pipeline():
    
    @task
    def extract() -> dict:
        """Extract data from API."""
        return {"data": [1, 2, 3]}
    
    @task
    def transform(data: dict) -> dict:
        """Transform data."""
        return {"transformed": data["data"]}
    
    @task
    def load(data: dict):
        """Load to database."""
        print(f"Loading: {data}")
    
    # Auto XCom communication!
    data = extract()
    transformed = transform(data)
    load(transformed)

# Instantiate DAG
my_etl_pipeline()
```

**Why TaskFlow API?**
- âœ… Less boilerplate code
- âœ… Automatic XCom handling
- âœ… Type safety dengan hints
- âœ… Cleaner, more Pythonic

---

#### **3. XCom Communication**
Folder: `3-xcom-patterns/`

**Topics:**
- âœ… Explicit XCom push/pull
- âœ… TaskFlow automatic XCom
- âœ… XCom limitations (size, serialization)
- âœ… Best practices (when to use, when to avoid)

**Explicit XCom:**
```python
def extract(**context):
    data = fetch_api()
    context['ti'].xcom_push(key='raw_data', value=data)

def transform(**context):
    data = context['ti'].xcom_pull(key='raw_data', task_ids='extract_task')
    transformed = process(data)
    return transformed  # Auto push dengan return
```

**TaskFlow XCom (Automatic):**
```python
@task
def extract() -> dict:
    return {"data": [1, 2, 3]}  # Auto XCom push

@task
def transform(data: dict) -> dict:  # Auto XCom pull
    return {"result": data}
```

---

#### **4. Task Scheduling & Retry**
Folder: `4-scheduling-retry/`

**Topics:**
- âœ… Schedule expressions (`@daily`, `@hourly`, cron)
- âœ… Retry mechanisms
- âœ… Exponential backoff
- âœ… Timeout handling
- âœ… SLA & alerting

**Example:**
```python
@dag(
    dag_id='resilient_pipeline',
    schedule='0 2 * * *',  # 2 AM daily
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args={
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
        'retry_exponential_backoff': True,
        'max_retry_delay': timedelta(hours=1),
        'execution_timeout': timedelta(hours=2)
    }
)
def my_dag():
    @task
    def risky_task():
        # Will retry 3x with exponential backoff
        api_call()
```

---

### ğŸŸ¡ **IMPORTANT: Advanced Topics**

#### **5. Environment Management**
Folder: `5-environment-config/`

**Topics:**
- âœ… Airflow Variables
- âœ… Connections (database, API)
- âœ… Environment variables dalam container
- âœ… Secret management
- âœ… Config separation (dev/prod)

**Using Variables:**
```python
from airflow.models import Variable

@task
def extract():
    api_key = Variable.get("API_KEY")
    api_url = Variable.get("API_URL")
    # Use in requests...
```

**Docker Compose Setup:**
```yaml
services:
  airflow-webserver:
    environment:
      - AIRFLOW_VAR_API_KEY=${API_KEY}
      - AIRFLOW_VAR_DATABASE_URL=${DATABASE_URL}
    env_file:
      - .env
```

---

#### **6. Import Path Resolution**
Folder: `6-import-resolution/`

**Topics:**
- âœ… PYTHONPATH dalam container
- âœ… Custom modules dalam DAGs
- âœ… Shared utilities
- âœ… Package structure best practices

**Project Structure:**
```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ my_dag.py
â”‚   â””â”€â”€ utils/          # âŒ Bad: tidak bisa import
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ plugins/            # âœ… Good: Airflow auto-load
â”‚   â””â”€â”€ custom_operators/
â””â”€â”€ include/            # âœ… Good: shared code
    â””â”€â”€ utils/
        â””â”€â”€ helpers.py
```

**Import in DAG:**
```python
# dags/my_dag.py
import sys
from pathlib import Path

# Add include to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'include'))

from utils.helpers import process_data  # Now works!
```

---

#### **7. Custom vs Built-in Operators**
Folder: `7-operators-comparison/`

**Topics:**
- âœ… When to use PythonOperator vs custom
- âœ… Creating custom operators
- âœ… Operator reusability
- âœ… Testing custom operators

**Built-in:**
```python
from airflow.operators.python import PythonOperator

task = PythonOperator(
    task_id='my_task',
    python_callable=my_function
)
```

**Custom Operator:**
```python
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults

class MyCustomOperator(BaseOperator):
    @apply_defaults
    def __init__(self, my_param, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.my_param = my_param
    
    def execute(self, context):
        # Custom logic
        print(f"Processing {self.my_param}")

# Usage
task = MyCustomOperator(task_id='custom', my_param='value')
```

---

#### **8. Background vs Blocking Tasks**
Folder: `8-task-execution-patterns/`

**Topics:**
- âœ… Synchronous vs asynchronous tasks
- âœ… Sensors for waiting
- âœ… Triggering external processes
- âœ… Deferrable operators

**Blocking (Default):**
```python
@task
def process_large_file():
    # Blocks until complete
    result = heavy_computation()
    return result
```

**Non-blocking (Sensor):**
```python
from airflow.sensors.filesystem import FileSensor

wait_for_file = FileSensor(
    task_id='wait_file',
    filepath='/data/input.csv',
    poke_interval=60,  # Check every 60s
    timeout=3600  # Give up after 1 hour
)
```

---

#### **9. Multi-Container Orchestration**
Folder: `9-docker-compose-setup/`

**Topics:**
- âœ… Airflow + PostgreSQL + Redis
- âœ… Volume mapping untuk DAGs
- âœ… Network configuration
- âœ… Resource limits
- âœ… Development vs Production setup

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  redis:
    image: redis:latest

  airflow-webserver:
    image: apache/airflow:2.8.0
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"

  airflow-scheduler:
    image: apache/airflow:2.8.0
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
```

---

## ğŸ“ Learning Path Recommendations

### Untuk Pemula:
1. Start dengan **DAG Fundamentals** (folder 1)
2. Learn **TaskFlow API** (folder 2) - modern approach
3. Understand **XCom** (folder 3) untuk inter-task communication
4. Practice **Scheduling & Retry** (folder 4)

**Goal:** Bisa bikin basic ETL pipeline dengan Airflow

---

### Untuk Intermediate:
5. Master **Environment Config** (folder 5)
6. Solve **Import Path** issues (folder 6)
7. Learn **Custom Operators** (folder 7)
8. Understand **Task Execution** patterns (folder 8)

**Goal:** Production-ready pipelines dengan proper configuration

---

### Untuk Advanced:
9. Setup **Multi-Container** environment (folder 9)
10. Implement monitoring & alerting
11. Performance optimization
12. CI/CD untuk DAGs

**Goal:** Scalable, maintainable Airflow deployment

---

## ğŸ“ Folder Structure

```
phase3-airflow/
â”œâ”€â”€ README.md (this file)
â”œâ”€â”€ 1-dag-fundamentals/
â”‚   â”œâ”€â”€ 1-first-dag.md
â”‚   â”œâ”€â”€ 2-task-dependencies.py
â”‚   â”œâ”€â”€ 3-basic-operators.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 2-taskflow-api/
â”‚   â”œâ”€â”€ 1-taskflow-intro.md
â”‚   â”œâ”€â”€ 2-taskflow-vs-traditional.py
â”‚   â”œâ”€â”€ 3-type-hints-xcom.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 3-xcom-patterns/
â”‚   â”œâ”€â”€ 1-xcom-basics.md
â”‚   â”œâ”€â”€ 2-explicit-xcom.py
â”‚   â”œâ”€â”€ 3-taskflow-xcom.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 4-scheduling-retry/
â”‚   â”œâ”€â”€ 1-schedule-expressions.md
â”‚   â”œâ”€â”€ 2-retry-mechanism.py
â”‚   â”œâ”€â”€ 3-timeout-handling.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 5-environment-config/
â”‚   â”œâ”€â”€ 1-variables-connections.md
â”‚   â”œâ”€â”€ 2-docker-env.py
â”‚   â”œâ”€â”€ 3-secret-management.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 6-import-resolution/
â”‚   â”œâ”€â”€ 1-pythonpath-issues.md
â”‚   â”œâ”€â”€ 2-project-structure.md
â”‚   â”œâ”€â”€ 3-import-solutions.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 7-operators-comparison/
â”‚   â”œâ”€â”€ 1-builtin-operators.md
â”‚   â”œâ”€â”€ 2-custom-operator.py
â”‚   â”œâ”€â”€ 3-testing-operators.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ 8-task-execution-patterns/
â”‚   â”œâ”€â”€ 1-sync-vs-async.md
â”‚   â”œâ”€â”€ 2-sensors.py
â”‚   â”œâ”€â”€ 3-deferrable-operators.py
â”‚   â””â”€â”€ README.md
â””â”€â”€ 9-docker-compose-setup/
    â”œâ”€â”€ 1-multi-container.md
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ docker-compose.dev.yml
    â”œâ”€â”€ docker-compose.prod.yml
    â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Basic DAG (Traditional)
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def my_task():
    print("Hello Airflow!")

with DAG(
    dag_id='hello_world',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    task = PythonOperator(
        task_id='say_hello',
        python_callable=my_task
    )
```

### 2. Modern DAG (TaskFlow API)
```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='hello_world_taskflow',
    schedule='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
)
def hello_pipeline():
    
    @task
    def say_hello():
        print("Hello from TaskFlow!")
        return "Success"
    
    say_hello()

hello_pipeline()
```

---

## ğŸ”— Integration dengan Tools Lain

### Airflow + Cloud Storage (GCS/S3)
```python
@task
def upload_to_gcs():
    from google.cloud import storage
    client = storage.Client()
    bucket = client.bucket('my-bucket')
    blob = bucket.blob('data.csv')
    blob.upload_from_filename('/tmp/data.csv')
```

### Airflow + dbt
```python
from airflow.operators.bash import BashOperator

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='cd /dbt && dbt run --profiles-dir .'
)
```

### Airflow + Spark
```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_job = SparkSubmitOperator(
    task_id='spark_transform',
    application='/path/to/spark_job.py',
    conn_id='spark_default'
)
```

---

## ğŸ¯ Best Practices

### âœ… DO's:
- Use **TaskFlow API** untuk DAGs baru
- Implement **retry mechanisms**
- Use **Variables** untuk configuration
- Test DAGs sebelum deploy
- Monitor task duration & failures
- Use **catchup=False** untuk development

### âŒ DON'Ts:
- Jangan hardcode credentials
- Jangan use `depends_on_past=True` tanpa alasan jelas
- Jangan buat tasks terlalu granular (overhead)
- Jangan ignore error handling
- Jangan use XCom untuk large data (use external storage)

---

## ğŸ“š Resources

### Official Docs:
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [TaskFlow API](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- [Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)

### Community:
- [Airflow Slack](https://apache-airflow.slack.com/)
- [GitHub Issues](https://github.com/apache/airflow/issues)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/airflow)

---

## ğŸ“ Next Steps

Setelah menguasai Airflow:
1. **Phase 4: Cloud** - Deploy Airflow ke GCP/AWS
2. **Phase 5: dbt** - Integrate dengan transformation tool
3. **Phase 6: Kafka** - Real-time data streaming
4. **Phase 7: Spark** - Large-scale data processing

---

**Happy Orchestrating! ğŸš€**
