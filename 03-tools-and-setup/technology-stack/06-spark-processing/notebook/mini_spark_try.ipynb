{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15467aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesAggregator\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258105ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# # 1. Konfigurasi\n",
    "# num_rows = 1_000_000\n",
    "# categories = ['Electronics', 'Clothing', 'Food', 'Books']\n",
    "# regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "# # 2. Buat \"Container\" (Range)\n",
    "# # Spark akan membagi 1 juta ini ke dalam beberapa partisi secara otomatis\n",
    "# df = spark.range(0, num_rows)\n",
    "\n",
    "# # 3. Transformasi menjadi Data Sales (Native Spark)\n",
    "# df_sales = df.withColumn(\n",
    "#         \"transaction_id\", \n",
    "#         F.concat(F.lit(\"TXN\"), F.lpad(F.col(\"id\").cast(\"string\"), 6, \"0\"))\n",
    "#     ).withColumn(\n",
    "#         \"category\", \n",
    "#         F.element_at(F.array([F.lit(c) for c in categories]), \n",
    "#                     (F.rand() * len(categories) + 1).cast(\"int\"))\n",
    "#     ).withColumn(\n",
    "#         \"region\", \n",
    "#         F.element_at(F.array([F.lit(r) for r in regions]), \n",
    "#                     (F.rand() * len(regions) + 1).cast(\"int\"))\n",
    "#     ).withColumn(\n",
    "#         \"amount\", \n",
    "#         F.round(F.rand() * (1000 - 10) + 10, 2)\n",
    "#     ).withColumn(\n",
    "#         \"quantity\", \n",
    "#         (F.rand() * 9 + 1).cast(\"int\")\n",
    "#     ).withColumn(\n",
    "#         \"date\", \n",
    "#         F.date_add(F.to_date(F.lit(\"2024-01-01\")), (F.rand() * 365).cast(\"int\"))\n",
    "#     ).drop(\"id\") # Buang kolom id bawaan spark.range\n",
    "\n",
    "# # 4. Write ke CSV menggunakan Engine Spark\n",
    "# # Ini akan jauh lebih cepat karena setiap executor menulis filenya sendiri\n",
    "# df_sales.write.mode(\"overwrite\").csv(\"/tmp/sales_data\", header=True)\n",
    "\n",
    "# print(\"Data 1 juta baris berhasil di-generate secara paralel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "])\n",
    "\n",
    "try:\n",
    "    df = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"FAILFAST\") \\\n",
    "        .schema(sales_schema) \\\n",
    "        .csv(\"/tmp/sales_data\", header = True)\n",
    "    \n",
    "    print(f\"Loaded {df.count()} records successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\",to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"date\")))\n",
    "\n",
    "nulls_count = df.select([ \\\n",
    "    count(when(col(c).isNull(), c)).alias(c) \\\n",
    "    for c in df.columns])\n",
    "\n",
    "print(\"Nulls count per column:\")\n",
    "nulls_count.show()\n",
    "\n",
    "print(\"\\nData Ranges:\")\n",
    "df.select(\n",
    "    min(\"amount\").alias(\"min_amount\"),\n",
    "    max(\"amount\").alias(\"max_amount\"),\n",
    "    min(\"quantity\").alias(\"min_quantity\"),\n",
    "    max(\"quantity\").alias(\"max_quantity\"),\n",
    "    min(\"date\").alias(\"min_date\"),\n",
    "    max(\"date\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52942ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10 contoh data\n",
    "print(\"Sample Data:\")\n",
    "df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_sales = df.groupBy(\"category\", \"region\" ,\"year\", \"month\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        count(\"transaction_id\").alias(\"transactions_count\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_value\")\n",
    "    )\\\n",
    "    .orderBy(\"year\", \"month\", \"category\", \"region\")\n",
    "\n",
    "print(\"Monthly Sales Aggregation:\")\n",
    "monthly_sales.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba0f41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enriched sales with ranking:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "|   category|region|year|month|     total_revenue|total_quantity|transactions_count|avg_transaction_value|region_rank|  cumulative_revenue|\n",
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "|       Food|  East|2024|    1|2663134.3899999987|         26304|              5282|     504.190531995456|          2|         1.0636074E7|\n",
      "|      Books|  East|2024|    2| 2487168.069999999|         24841|              4986|   498.83033894905714|          2|1.3123242069999998E7|\n",
      "|      Books|  East|2024|    3|2730219.0999999987|         26858|              5352|   510.13062406576955|          1|2.3176740029999994E7|\n",
      "|   Clothing|  East|2024|    3|2764524.6899999995|         27150|              5421|   509.96581627006077|          1| 2.594126471999999E7|\n",
      "|Electronics|  East|2024|    3|2693562.6799999997|         26858|              5362|   502.34290936217826|          2| 2.863482739999999E7|\n",
      "|      Books|  East|2024|    4| 2612875.590000001|         26114|              5213|   501.22301745635923|          1| 3.390657822999999E7|\n",
      "|   Clothing|  East|2024|    4|2629216.5600000005|         25955|              5241|    501.6631482541501|          2| 3.653579478999999E7|\n",
      "|      Books|  East|2024|    6|2653183.9000000013|         25941|              5194|    510.8170773969968|          1|5.4854685069999985E7|\n",
      "|   Clothing|  East|2024|    6| 2629947.869999999|         26388|              5216|    504.2077971625765|          1| 5.748463293999998E7|\n",
      "|Electronics|  East|2024|    7|2699690.0899999994|         26747|              5327|    506.7937094049182|          1| 7.062750253999999E7|\n",
      "|       Food|  East|2024|    7|2707754.6299999994|         26682|              5361|   505.08387054653974|          2| 7.333525716999999E7|\n",
      "|      Books|  East|2024|    8|2740654.7800000003|         26935|              5396|    507.9048888065234|          2| 7.607591194999999E7|\n",
      "|Electronics|  East|2024|    8|2687104.4899999993|         26624|              5314|    505.6651279638689|          1| 8.143128796999998E7|\n",
      "|       Food|  East|2024|    8|2718839.7300000004|         27197|              5388|   504.61019487750565|          2| 8.415012769999999E7|\n",
      "|   Clothing|  East|2024|    9| 2602342.200000002|         25984|              5174|    502.9652493235412|          1|       8.930912605E7|\n",
      "|       Food|  East|2024|    9|        2621318.22|         25809|              5154|    508.5988009313155|          2| 9.451434442999999E7|\n",
      "|   Clothing|  East|2024|   10| 2749323.189999999|         26692|              5342|    514.6617727442904|          1| 9.990671772999999E7|\n",
      "|Electronics|  East|2024|   10|2679366.1900000023|         26592|              5314|   504.20891795257853|          1|1.0258608391999999E8|\n",
      "|       Food|  East|2024|   10| 2737925.290000002|         26817|              5358|    510.9976278462116|          1|      1.0532400921E8|\n",
      "|      Books|  East|2024|   11|2643222.8999999994|         26468|              5248|    503.6629001524389|          2|      1.0796723211E8|\n",
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Window untuk ranking per category\n",
    "window_category = Window.partitionBy(\"category\",\"year\",\"month\")\\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# Window untuk running total per region\n",
    "window_running = Window.partitionBy(\"region\") \\\n",
    "    .orderBy(\"year\", \"month\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "enriched_sales = monthly_sales \\\n",
    "    .withColumn(\"region_rank\", rank().over(window_category)) \\\n",
    "    .withColumn(\"cumulative_revenue\", sum(\"total_revenue\").over(window_running))\n",
    "\n",
    "print(\"\\nEnriched sales with ranking:\")\n",
    "enriched_sales.filter(col(\"region_rank\") <= 2).show(20)\n",
    "\n",
    "# buatkan penjelasan singkat tentang apa yang dilakukan window_function di atas\n",
    "# Penjelasan:\n",
    "# 1. window_category: Membuat jendela (window) yang mengelompokan data berdasarkan kategori, tahun, dan bulan. Data di dalam jendela ini diurutkan berdasarkan total pendapatan (total_revenue) secara menurun. Fungsi ini digunakan untuk memberikan peringkat (rank) pada setiap kategori dalam setiap bulan berdasarkan pendapatan tertinggi.\n",
    "# 2. window_running: Membuat jendela yang mengelompokan data berdasarkan wilayah (region). Data di dalam jendela ini diurutkan berdasarkan tahun dan bulan. Fungsi ini digunakan untuk menghitung total kumulatif (cumulative sum) dari pendapatan (total_revenue) dari awal hingga baris saat ini dalam setiap wilayah. Dengan kata lain, ini memberikan gambaran tentang bagaimana pendapatan bertambah seiring waktu di setiap wilayah.\n",
    "# 3. enriched_sales: DataFrame ini menambahkan dua kolom baru ke dalam hasil agregasi bulanan. Kolom \"region_rank\" memberikan peringkat untuk setiap kategori dalam setiap bulan berdasarkan total pendapatan, sedangkan kolom \"cumulative_revenue\" menghitung total pendapatan kumulatif untuk setiap wilayah dari waktu ke waktu. Hasil akhirnya adalah DataFrame yang lebih informatif yang memungkinkan analisis lebih lanjut berdasarkan peringkat dan tren pendapatan kumulatif.\n",
    "\n",
    "# apa bedanya jendela dan groupby di spark?\n",
    "# Perbedaan utama antara jendela (window) dan groupBy di Spark adalah cara mereka mengelompokkan dan memproses data:\n",
    "# 1. groupBy: Mengelompokkan data berdasarkan satu atau lebih kolom dan kemudian menerapkan agregasi (seperti sum, avg, count) pada setiap kelompok. Hasilnya adalah satu baris per kelompok.\n",
    "# 2. Jendela (Window): Membuat jendela yang memungkinkan operasi agregasi dilakukan pada subset data yang terkait dengan setiap baris, tanpa mengurangi jumlah baris dalam DataFrame. Ini memungkinkan perhitungan seperti peringkat, total kumulatif, dan rata-rata berjalan, di mana setiap baris tetap ada dalam hasil akhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bb0c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated data written to /tmp/sales_aggregated_parquet with partitioning by year and month.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write sebagai Parquet dengan partitioning\n",
    "output_path = \"/tmp/sales_aggregated_parquet\"\n",
    "enriched_sales.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(f\"Aggregated data written to {output_path} with partitioning by year and month.\")\n",
    "\n",
    "# Untuk dataset kecil, lebih baik NO partitioning\n",
    "enriched_sales.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0f9dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification:\n",
      "Output record count: 192\n",
      "Output schema:\n",
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- total_revenue: double (nullable = true)\n",
      " |-- total_quantity: long (nullable = true)\n",
      " |-- transactions_count: long (nullable = true)\n",
      " |-- avg_transaction_value: double (nullable = true)\n",
      " |-- region_rank: integer (nullable = true)\n",
      " |-- cumulative_revenue: double (nullable = true)\n",
      "\n",
      "\n",
      "Sample output:\n",
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "|category   |region|year|month|total_revenue     |total_quantity|transactions_count|avg_transaction_value|region_rank|cumulative_revenue  |\n",
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "|Food       |South |2024|1    |2718987.1499999976|26679         |5377              |505.6699181699828    |1          |1.0740537409999996E7|\n",
      "|Books      |West  |2024|1    |2707757.39        |26846         |5348              |506.31215220643236   |1          |2707757.39          |\n",
      "|Clothing   |South |2024|1    |2705727.7299999995|26494         |5345              |506.2166005612721    |1          |5370808.389999997   |\n",
      "|Clothing   |North |2024|1    |2697276.4899999993|26325         |5253              |513.4735370264609    |2          |5394221.15          |\n",
      "|Books      |North |2024|1    |2696944.660000001 |26374         |5303              |508.56961342636265   |2          |2696944.660000001   |\n",
      "|Books      |East  |2024|1    |2681760.74        |26988         |5309              |505.13481634959504   |3          |2681760.74          |\n",
      "|Electronics|West  |2024|1    |2674752.1199999996|26196         |5280              |506.58184090909083   |1          |8024431.220000001   |\n",
      "|Electronics|North |2024|1    |2673252.38        |26479         |5272              |507.0660811836115    |2          |8067473.53          |\n",
      "|Books      |South |2024|1    |2665080.659999998 |26202         |5276              |505.1328013646698    |4          |2665080.659999998   |\n",
      "|Food       |East  |2024|1    |2663134.3899999987|26304         |5282              |504.190531995456     |2          |1.0636074E7         |\n",
      "+-----------+------+----+-----+------------------+--------------+------------------+---------------------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "sales_aggregated_parquet/\n",
      "  part-00000-ba0e2494-5992-43a3-af4f-be7d4ed653a4-c000.snappy.parquet\n",
      "  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Read back dan verify\n",
    "result_df = spark.read.parquet(output_path)\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Output record count: {result_df.count()}\")\n",
    "print(f\"Output schema:\")\n",
    "result_df.printSchema()\n",
    "\n",
    "# Spot check beberapa records\n",
    "print(\"\\nSample output:\")\n",
    "result_df.orderBy(\"year\", \"month\", \"total_revenue\", ascending=[True, True, False]) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# Check file layout\n",
    "import os\n",
    "for root, dirs, files in os.walk(output_path):\n",
    "    level = root.replace(output_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        if not file.startswith('.'):\n",
    "            print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99266b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Physical plan:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (15)\n",
      "+- Window (14)\n",
      "   +- Sort (13)\n",
      "      +- Exchange (12)\n",
      "         +- Window (11)\n",
      "            +- Sort (10)\n",
      "               +- Exchange (9)\n",
      "                  +- Sort (8)\n",
      "                     +- Exchange (7)\n",
      "                        +- HashAggregate (6)\n",
      "                           +- Exchange (5)\n",
      "                              +- HashAggregate (4)\n",
      "                                 +- Project (3)\n",
      "                                    +- Project (2)\n",
      "                                       +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [6]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, date#123]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/tmp/sales_data]\n",
      "ReadSchema: struct<transaction_id:string,category:string,region:string,amount:double,quantity:int,date:string>\n",
      "\n",
      "(2) Project\n",
      "Output [6]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, cast(gettimestamp(date#123, yyyy-MM-dd, TimestampType, try_to_date, Some(Asia/Jakarta), true) as date) AS date#135]\n",
      "Input [6]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, date#123]\n",
      "\n",
      "(3) Project\n",
      "Output [7]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, year(date#135) AS year#136, month(date#135) AS month#137]\n",
      "Input [6]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, date#135]\n",
      "\n",
      "(4) HashAggregate\n",
      "Input [7]: [transaction_id#118, category#119, region#120, amount#121, quantity#122, year#136, month#137]\n",
      "Keys [4]: [category#119, region#120, year#136, month#137]\n",
      "Functions [4]: [partial_sum(amount#121), partial_sum(quantity#122), partial_count(transaction_id#118), partial_avg(amount#121)]\n",
      "Aggregate Attributes [5]: [sum#470, sum#471L, count#472L, sum#473, count#474L]\n",
      "Results [9]: [category#119, region#120, year#136, month#137, sum#475, sum#476L, count#477L, sum#478, count#479L]\n",
      "\n",
      "(5) Exchange\n",
      "Input [9]: [category#119, region#120, year#136, month#137, sum#475, sum#476L, count#477L, sum#478, count#479L]\n",
      "Arguments: hashpartitioning(category#119, region#120, year#136, month#137, 4), ENSURE_REQUIREMENTS, [plan_id=2702]\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [9]: [category#119, region#120, year#136, month#137, sum#475, sum#476L, count#477L, sum#478, count#479L]\n",
      "Keys [4]: [category#119, region#120, year#136, month#137]\n",
      "Functions [4]: [sum(amount#121), sum(quantity#122), count(transaction_id#118), avg(amount#121)]\n",
      "Aggregate Attributes [4]: [sum(amount#121)#458, sum(quantity#122)#459L, count(transaction_id#118)#460L, avg(amount#121)#461]\n",
      "Results [8]: [category#119, region#120, year#136, month#137, sum(amount#121)#458 AS total_revenue#446, sum(quantity#122)#459L AS total_quantity#447L, count(transaction_id#118)#460L AS transactions_count#448L, avg(amount#121)#461 AS avg_transaction_value#449]\n",
      "\n",
      "(7) Exchange\n",
      "Input [8]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449]\n",
      "Arguments: rangepartitioning(year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST, category#119 ASC NULLS FIRST, region#120 ASC NULLS FIRST, 4), ENSURE_REQUIREMENTS, [plan_id=2705]\n",
      "\n",
      "(8) Sort\n",
      "Input [8]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449]\n",
      "Arguments: [year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST, category#119 ASC NULLS FIRST, region#120 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(9) Exchange\n",
      "Input [8]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449]\n",
      "Arguments: hashpartitioning(category#119, year#136, month#137, 4), ENSURE_REQUIREMENTS, [plan_id=2708]\n",
      "\n",
      "(10) Sort\n",
      "Input [8]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449]\n",
      "Arguments: [category#119 ASC NULLS FIRST, year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST, total_revenue#446 DESC NULLS LAST], false, 0\n",
      "\n",
      "(11) Window\n",
      "Input [8]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449]\n",
      "Arguments: [rank(total_revenue#446) windowspecdefinition(category#119, year#136, month#137, total_revenue#446 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS region_rank#722], [category#119, year#136, month#137], [total_revenue#446 DESC NULLS LAST]\n",
      "\n",
      "(12) Exchange\n",
      "Input [9]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449, region_rank#722]\n",
      "Arguments: hashpartitioning(region#120, 4), ENSURE_REQUIREMENTS, [plan_id=2712]\n",
      "\n",
      "(13) Sort\n",
      "Input [9]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449, region_rank#722]\n",
      "Arguments: [region#120 ASC NULLS FIRST, year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST], false, 0\n",
      "\n",
      "(14) Window\n",
      "Input [9]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449, region_rank#722]\n",
      "Arguments: [sum(total_revenue#446) windowspecdefinition(region#120, year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS cumulative_revenue#731], [region#120], [year#136 ASC NULLS FIRST, month#137 ASC NULLS FIRST]\n",
      "\n",
      "(15) AdaptiveSparkPlan\n",
      "Output [10]: [category#119, region#120, year#136, month#137, total_revenue#446, total_quantity#447L, transactions_count#448L, avg_transaction_value#449, region_rank#722, cumulative_revenue#731]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lihat execution plan\n",
    "print(\"\\nPhysical plan:\")\n",
    "enriched_sales.explain(mode=\"formatted\")\n",
    "\n",
    "# Cleanup\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6533e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
